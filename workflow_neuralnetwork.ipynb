{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aa7d8fb-57d7-4326-999d-f97d98f5977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a23ece-af0e-4a57-ad07-1b7e9f7e61ca",
   "metadata": {},
   "source": [
    "6,915 normal cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "873df1e5-6bca-4808-a758-ebf5e2e6ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in regulon activity matrix\n",
    "dta_nn = pd.read_pickle(\"/n/holyscratch01/stephenson_lab/Users/taylerli/SCENIC/data/BM_normal_output/BM_normal_aucell_values.pkl.gz\")\n",
    "# read in correct annotations\n",
    "anno_names = os.path.join(\"/n/holyscratch01/stephenson_lab/Users/taylerli/SCENIC/data\", \"GSM*_BM*.anno.txt.gz\")\n",
    "anno_names = glob.glob(anno_names)\n",
    "anno_mat = pd.read_csv(anno_names[0], sep='\\t', header=0, index_col=0)\n",
    "for i in range(1, len(anno_names)):\n",
    "    anno_mat = pd.concat([anno_mat, pd.read_csv(anno_names[i], sep='\\t', header=0, index_col=0)], axis = 0)\n",
    "# join annotation to regulon activity matrix\n",
    "dta_nn = pd.merge(dta_nn, anno_mat['CellType'], on = 'Cell', how = 'left')\n",
    "\n",
    "# partition out training set, validation set, and test set using stratified sampling\n",
    "np.random.seed(292876)\n",
    "train_set = dta_nn.groupby('CellType', group_keys=False).apply(lambda x: x.sample(frac=0.7))\n",
    "train_set = train_set.sample(frac = 1)\n",
    "validation_set = dta_nn.drop(train_set.index).groupby('CellType', group_keys=False).apply(lambda x: x.sample(frac=1/3))\n",
    "test_set = dta_nn.drop(train_set.index).drop(validation_set.index)\n",
    "\n",
    "# separate X train from y train\n",
    "X_train = train_set.drop('CellType', axis = 1)\n",
    "y_train = train_set['CellType']\n",
    "\n",
    "# get X validation and y validation\n",
    "X_validation = validation_set.drop('CellType', axis = 1)\n",
    "y_validation = validation_set['CellType']\n",
    "\n",
    "# get X test and y test\n",
    "X_test = test_set.drop('CellType', axis = 1)\n",
    "y_test = test_set['CellType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21fc6dd1-0906-4fda-acf9-55aaf2623637",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_pickle(\"train_set.pkl.gz\")\n",
    "validation_set.to_pickle(\"validation_set.pkl.gz\")\n",
    "test_set.to_pickle(\"test_set.pkl.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4b852-99c4-4561-ae70-c5631870fa70",
   "metadata": {},
   "source": [
    "20,148 healthy and cancer cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1937d16c-3638-497f-a8ac-870dc48ba547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in regulon activity matrix\n",
    "dta_nn = pd.read_pickle(\"/n/holyscratch01/stephenson_lab/Users/taylerli/SCENIC/data/BM_AML_normal_output/BM_AML_normal_aucell_values.pkl.gz\")\n",
    "# read in correct annotations\n",
    "anno_names = os.path.join(\"/n/holyscratch01/stephenson_lab/Users/taylerli/SCENIC/data\", \"GSM*_*.anno.txt.gz\")\n",
    "anno_names = glob.glob(anno_names)\n",
    "anno_mat = pd.read_csv(anno_names[0], sep='\\t', header=0, index_col=0)\n",
    "for i in range(1, len(anno_names)):\n",
    "    anno_mat = pd.concat([anno_mat, pd.read_csv(anno_names[i], sep='\\t', header=0, index_col=0)], axis = 0)\n",
    "# join annotation to regulon activity matrix\n",
    "dta_nn = pd.merge(dta_nn, anno_mat['CellType'], on = 'Cell', how = 'left')\n",
    "dta_nn['HC'] = np.where(dta_nn.CellType.str[-4:] == 'like', 'Cancer', 'Healthy')\n",
    "# drop unuseful regulons, according to LASSO\n",
    "dta_nn = dta_nn.drop(dta_nn.columns[[11, 45, 63, 67, 77, 86, 108, 134, 158, 218, 219, 241, 265, 269, 297, 303, 312]], axis = 1)\n",
    "\n",
    "# partition out training set, validation set, and test set using stratified sampling\n",
    "np.random.seed(292876)\n",
    "train_set = dta_nn.groupby('CellType', group_keys=False).apply(lambda x: x.sample(frac=0.7))\n",
    "train_set = train_set.sample(frac = 1)\n",
    "validation_set = dta_nn.drop(train_set.index).groupby('CellType', group_keys=False).apply(lambda x: x.sample(frac=1/3))\n",
    "test_set = dta_nn.drop(train_set.index).drop(validation_set.index)\n",
    "\n",
    "# separate X train from y train\n",
    "X_train = train_set.drop(['CellType', 'HC'], axis = 1)\n",
    "y_train = train_set['HC']\n",
    "\n",
    "# get X validation and y validation\n",
    "X_validation = validation_set.drop(['CellType', 'HC'], axis = 1)\n",
    "y_validation = validation_set['HC']\n",
    "\n",
    "# get X test and y test\n",
    "X_test = test_set.drop(['CellType', 'HC'], axis = 1)\n",
    "y_test = test_set['HC']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f664d6-84fc-4eed-95a1-e98078fabafc",
   "metadata": {},
   "source": [
    "6,905 cancer cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a66d5f6-3535-41b6-87be-51e0354fc5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in regulon activity matrix\n",
    "dta_nn = pd.read_pickle(\"/n/holyscratch01/stephenson_lab/Users/taylerli/SCENIC/data/BM_AML_normal_output/BM_AML_normal_aucell_values.pkl.gz\")\n",
    "# read in correct annotations\n",
    "anno_names = os.path.join(\"/n/holyscratch01/stephenson_lab/Users/taylerli/SCENIC/data\", \"GSM*_*.anno.txt.gz\")\n",
    "anno_names = glob.glob(anno_names)\n",
    "anno_mat = pd.read_csv(anno_names[0], sep='\\t', header=0, index_col=0)\n",
    "for i in range(1, len(anno_names)):\n",
    "    anno_mat = pd.concat([anno_mat, pd.read_csv(anno_names[i], sep='\\t', header=0, index_col=0)], axis = 0)\n",
    "# join annotation to regulon activity matrix\n",
    "dta_nn = pd.merge(dta_nn, anno_mat['CellType'], on = 'Cell', how = 'left')\n",
    "dta_nn['HC'] = np.where(dta_nn.CellType.str[-4:] == 'like', 'Cancer', 'Healthy')\n",
    "\n",
    "\n",
    "dta_nn1 = dta_nn[dta_nn['HC'] == 'Cancer']\n",
    "train_set = dta_nn1.groupby('CellType', group_keys=False).apply(lambda x: x.sample(frac=0.7))\n",
    "train_set = train_set.sample(frac = 1)\n",
    "validation_set = dta_nn1.drop(train_set.index).groupby('CellType', group_keys=False).apply(lambda x: x.sample(frac=1/3))\n",
    "test_set = dta_nn1.drop(train_set.index).drop(validation_set.index)\n",
    "\n",
    "# separate X train from y train\n",
    "X_train = train_set.drop(['CellType', 'HC'], axis = 1)\n",
    "y_train = train_set['CellType']\n",
    "\n",
    "# get X validation and y validation\n",
    "X_validation = validation_set.drop(['CellType', 'HC'], axis = 1)\n",
    "y_validation = validation_set['CellType']\n",
    "\n",
    "# get X test and y test\n",
    "X_test = test_set.drop(['CellType', 'HC'], axis = 1)\n",
    "y_test = test_set['CellType']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd61045-7e5a-464d-9c5d-c1c4ea97b5ed",
   "metadata": {},
   "source": [
    "Regulon composition lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34703b87-5c72-4529-96e6-f137e6cb0d52",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIC2\n",
      "PSMD13\n",
      "MGAT1\n",
      "PDLIM2\n",
      "PRKCD\n",
      "EPN1\n",
      "NCF4\n",
      "KIAA0232\n",
      "LASP1\n",
      "PRKG1\n",
      "SV2C\n",
      "ARPC4\n",
      "RHOG\n",
      "IL2RA\n",
      "GPSM3\n",
      "CCND3\n",
      "ACTG1\n",
      "ARHGAP1\n",
      "PGS1\n",
      "CARD9\n",
      "RALB\n",
      "RAB31\n",
      "SSTR2\n",
      "SLC9A3R1\n",
      "SLC16A3\n",
      "CFP\n",
      "RTN4\n",
      "HLA-DPB2\n",
      "ARPC1B\n",
      "CD163L1\n",
      "TEX264\n",
      "RGS14\n",
      "RAB27B\n",
      "HLA-C\n",
      "TWF2\n",
      "XAF1\n",
      "CAP1\n",
      "MPG\n",
      "LSP1\n",
      "HEXIM1\n",
      "CSK\n",
      "CAMK1D\n",
      "ANTXR2\n",
      "DPEP2\n",
      "PFN1\n",
      "S100A10\n",
      "SLC9A3R2\n",
      "HPCAL1\n",
      "C2orf88\n",
      "NCALD\n",
      "TNNI2\n",
      "RAB11FIP1\n",
      "CTSB\n",
      "AKNA\n",
      "ARF1\n",
      "ADAM15\n",
      "LRRK1\n",
      "HLA-DQA1\n",
      "FILIP1L\n",
      "KIAA0513\n",
      "VPS4B\n",
      "SNAI3\n",
      "DLL4\n",
      "DGKG\n",
      "NAAA\n",
      "FGD2\n",
      "AP1G2\n",
      "URGCP\n",
      "HLA-DMB\n",
      "PLCB2\n",
      "LRCH4\n",
      "TNFAIP8L2\n",
      "MTMR14\n",
      "CD36\n",
      "ITGA2B\n",
      "CDON\n",
      "GNAL\n",
      "CDC42SE1\n",
      "SLAIN1\n",
      "UCP2\n",
      "TIAF1\n",
      "ERP29\n",
      "LTBP1\n",
      "TMBIM1\n",
      "PEA15\n",
      "CORO1B\n",
      "ARHGAP27\n",
      "ERCC1\n",
      "CD52\n",
      "RND1\n",
      "RELT\n",
      "LPXN\n",
      "RPS6KA1\n",
      "PPFIBP2\n",
      "GAS7\n",
      "ZMIZ1\n",
      "CORO1A\n",
      "OSBPL3\n",
      "PLEKHM1\n",
      "EMP1\n",
      "ARHGAP6\n",
      "TRIM16\n",
      "HLA-DRA\n",
      "PLEK\n",
      "GSDMD\n",
      "DPYSL3\n",
      "RAB1B\n",
      "PILRB\n",
      "CYTH4\n",
      "ARL2BP\n",
      "MSN\n",
      "ITGB7\n",
      "RALY\n",
      "C1QTNF2\n",
      "ITGA11\n",
      "GNB2\n",
      "C17orf62\n",
      "EMX2\n",
      "TNIP1\n",
      "PRR5L\n",
      "ALOX5\n",
      "NFE2L1\n",
      "ATP10D\n",
      "CAPN1\n",
      "ADRB2\n",
      "CALCRL\n",
      "DLGAP4\n",
      "PLCD4\n",
      "CTDSPL\n"
     ]
    }
   ],
   "source": [
    "def print0(x):\n",
    "    lis = [ele for ele in x]\n",
    "    for l in lis:\n",
    "        print(l)\n",
    "\n",
    "c = pd.read_pickle(\"/n/holyscratch01/stephenson_lab/Users/taylerli/SCENIC/data/BM_AML_normal_output/BM_AML_normal_regulons.p\")\n",
    "print0(c[[i for i in range(len(c)) if c[i].name == 'TAGLN2(+)'][0]].gene2weight.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a8865-e74f-41b1-b4a5-60adf5111101",
   "metadata": {},
   "source": [
    "Neural Network class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41382164-a8c5-4dab-aaed-7e30ab588ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, hidden = [50], act_func = ['sigmoid'], epochs = 10, lr = 0.1, iter_size = 4):\n",
    "        if len(hidden) != len(act_func):\n",
    "            raise ValueError('The number of hidden layers and the number of activation functions must be the same.')\n",
    "        self.__hidden = hidden\n",
    "        self.__act_func = act_func\n",
    "        self.__epochs = epochs\n",
    "        self.__lr = lr\n",
    "        self.__iter_size = iter_size\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.params = []\n",
    "        self.history_accuracy = []\n",
    "        self.history_loss = []\n",
    "    \n",
    "    def modify(self, hidden = None, act_func = None, epochs = None, lr = None, iter_size = None):\n",
    "        if hidden is not None:\n",
    "            if act_func is not None:\n",
    "                if len(hidden) != len(act_func):\n",
    "                    raise ValueError('The number of hidden layers and the number of activation functions must be the same.')\n",
    "                else:\n",
    "                    self.__hidden = hidden\n",
    "                    self.__act_func = act_func\n",
    "            else:\n",
    "                if len(hidden) != len(self.__act_func):\n",
    "                    raise ValueError('The number of hidden layers and the number of activation functions must be the same.')\n",
    "                else:\n",
    "                    self.__hidden = hidden\n",
    "        elif act_func is not None:\n",
    "            if len(self.__hidden) != len(act_func):\n",
    "                raise ValueError('The number of hidden layers and the number of activation functions must be the same.')\n",
    "            else:\n",
    "                self.__act_func = act_func\n",
    "        elif epochs is not None:\n",
    "            self.__epochs = epochs\n",
    "        elif lr is not None:\n",
    "            self.__lr = lr\n",
    "        elif iter_size is not None:\n",
    "            self.__iter_size = iter_size\n",
    "        else:\n",
    "            raise ValueError('Must make at least one modification!')\n",
    "            \n",
    "    def refresh(self):\n",
    "        self.params = []\n",
    "        self.history_accuracy = []\n",
    "        self.history_loss = []\n",
    "    \n",
    "    def fit(self, X, y, scale_X = True):\n",
    "        # getting training X and y\n",
    "        y = pd.get_dummies(y)    # one-hot encoding training y\n",
    "        self.y_train = torch.from_numpy(y.values).float()\n",
    "        if scale_X:\n",
    "            X = (X-X.min())/(X.max()-X.min())\n",
    "        self.X_train = torch.from_numpy(X.values).float()\n",
    "        \n",
    "        # initializing weights and bias\n",
    "        for i in range(len(self.__hidden)):\n",
    "            if i == 0:\n",
    "                W = torch.nn.Parameter(0.01 * torch.randn(size = (self.X_train.shape[1], self.__hidden[i])))\n",
    "            else:\n",
    "                W = torch.nn.Parameter(0.01 * torch.randn(size = (self.__hidden[i-1], self.__hidden[i])))\n",
    "            b = torch.nn.Parameter(torch.zeros(self.__hidden[i]))\n",
    "            self.params.append(W)\n",
    "            self.params.append(b)\n",
    "        W_final = torch.nn.Parameter(0.01 * torch.randn(size = (self.__hidden[len(self.__hidden)-1], self.y_train.shape[1])))\n",
    "        b_final = torch.nn.Parameter(torch.zeros(self.y_train.shape[1]))\n",
    "        self.params.append(W_final)\n",
    "        self.params.append(b_final)\n",
    "        \n",
    "    def __ReLU__(self, X):\n",
    "        return torch.clamp(X, min = 0)\n",
    "\n",
    "    def __leakyReLU__(self, X):\n",
    "        return torch.where(X >= 0, X, 0.1*X)\n",
    "\n",
    "    def __sigmoid__(self, X):\n",
    "        return 1 / (1 + torch.exp(-X))\n",
    "\n",
    "    def __tanh__(self, X):\n",
    "        return torch.tanh(X)\n",
    "\n",
    "    def __ELU__(self, X):\n",
    "        return torch.where(X >= 0, X, 0.1*(torch.exp(X) - 1))\n",
    "\n",
    "    def __softmax__(self, X):\n",
    "        e = torch.exp(X)\n",
    "        exp_sum = torch.sum(e, dim = 1, keepdim = True)\n",
    "        soft = e/exp_sum\n",
    "        return soft\n",
    "        \n",
    "    def __forwardPass__(self, X):\n",
    "        for j in range(len(self.params) // 2):\n",
    "            W = self.params[j*2]\n",
    "            b = self.params[j*2 + 1]\n",
    "            if j < len(self.params) // 2 - 1:\n",
    "                func = eval('self.__' + self.__act_func[j] + '__')\n",
    "                if j == 0:\n",
    "                    H = func(X@W + b)\n",
    "                else:\n",
    "                    H = func(H@W + b)\n",
    "            else:\n",
    "                O = self.__softmax__(H@W + b)\n",
    "        return O\n",
    "    \n",
    "    def __accuracy__(self, y_hat, y):\n",
    "        with torch.no_grad():\n",
    "            y_pred = y_hat.argmax(axis=1)\n",
    "            y_true = y.argmax(axis = 1)\n",
    "            correct = y_pred == y_true\n",
    "        return correct.sum() / correct.numel()\n",
    "        \n",
    "    def __crossEntropy__(self, y_hat, y):\n",
    "        ln_yhat = torch.log(y_hat)\n",
    "        cr_en = -torch.sum(y * ln_yhat, dim = 1)\n",
    "        return cr_en.mean()\n",
    "    \n",
    "    def __update__(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.params:\n",
    "                param -= self.__lr * param.grad\n",
    "                param.grad.zero_()\n",
    "        return\n",
    "        \n",
    "    def train(self):\n",
    "        if self.X_train == None:\n",
    "            raise ValueError('Must need a training set to start training.')\n",
    "        for _ in range(self.__epochs):\n",
    "            accuracy = 0.0\n",
    "            loss = 0.0\n",
    "            n_trials = 0\n",
    "            for i in range((self.X_train.shape[0] - 1) // self.__iter_size + 1):\n",
    "                n_trials += 1\n",
    "                X_iter = self.X_train[i*self.__iter_size:(i+1)*self.__iter_size, :]\n",
    "                y_iter = self.y_train[i*self.__iter_size:(i+1)*self.__iter_size, :]\n",
    "                y_hat = self.__forwardPass__(X_iter)\n",
    "                accuracy += self.__accuracy__(y_hat, y_iter)\n",
    "                l = self.__crossEntropy__(y_hat, y_iter)\n",
    "                loss += l\n",
    "                l.backward()\n",
    "                self.__update__()\n",
    "            self.history_accuracy.append(accuracy.item() / n_trials)\n",
    "            self.history_loss.append(loss.item())\n",
    "    \n",
    "    def get_accuracy(self, X = None, y = None, scale_X = True):\n",
    "        if X is not None and y is not None:\n",
    "            y = pd.get_dummies(y)\n",
    "            if scale_X:\n",
    "                X = (X-X.min())/(X.max()-X.min())\n",
    "            y_test = torch.from_numpy(y.values).float()\n",
    "            X_test = torch.from_numpy(X.values).float()\n",
    "            return self.__accuracy__(self.__forwardPass__(X_test), y_test).item()\n",
    "        else:\n",
    "            return self.__accuracy__(self.__forwardPass__(self.X_train), self.y_train).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecbd49f-60b4-4cb7-8366-9ac1e0466aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.9163222908973694\n",
      "The validation set accuracy is 0.8075253367424011\n",
      "The training set accuracy is 0.9661157131195068\n",
      "The validation set accuracy is 0.7539797425270081\n",
      "The training set accuracy is 0.016322314739227295\n",
      "The validation set accuracy is 0.01591895893216133\n",
      "The training set accuracy is 0.016322314739227295\n",
      "The validation set accuracy is 0.01591895893216133\n"
     ]
    }
   ],
   "source": [
    "for ele in ['sigmoid', 'tanh', 'ReLU', 'leakyReLU', 'ELU']:\n",
    "    NN = NeuralNetwork([50], [ele], epochs = 40, lr = 0.05)\n",
    "    NN.fit(X_train, y_train)\n",
    "    NN.train()\n",
    "    print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "    print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae9b568-4420-4f2d-ba34-3d3c3bf446a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.8444215059280396\n",
      "The validation set accuracy is 0.7973950505256653\n",
      "The training set accuracy is 0.9212809801101685\n",
      "The validation set accuracy is 0.8321273326873779\n",
      "The training set accuracy is 0.9179751873016357\n",
      "The validation set accuracy is 0.8263386487960815\n",
      "The training set accuracy is 0.9130165576934814\n",
      "The validation set accuracy is 0.8219971060752869\n",
      "The training set accuracy is 0.921074390411377\n",
      "The validation set accuracy is 0.8306801915168762\n"
     ]
    }
   ],
   "source": [
    "for ele in ['sigmoid', 'tanh', 'ReLU', 'leakyReLU', 'ELU']:\n",
    "    NN = NeuralNetwork([50], [ele], epochs = 40, lr = 0.05)\n",
    "    NN.fit(X_train, y_train, scale_X=False)\n",
    "    NN.train()\n",
    "    print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "    print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22cf473c-eb45-4c0f-9833-82cc318d5915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.8704545497894287\n",
      "The validation set accuracy is 0.7814761400222778\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([100, 50], ['tanh', 'ELU'], epochs = 40, lr = 0.05)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ceb055e-2d42-4597-a1f5-07aada865c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.8535124063491821\n",
      "The validation set accuracy is 0.80173659324646\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([100, 50], ['tanh', 'sigmoid'], epochs = 40, lr = 0.05)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a7e013-8cb3-44fb-a864-dafc9ebb054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.8962810039520264\n",
      "The validation set accuracy is 0.784370481967926\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([100, 50], ['ELU', 'tanh'], epochs = 40, lr = 0.05)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "face5e24-75d7-46cb-8d4c-e5716e4ed35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.016322314739227295\n",
      "The validation set accuracy is 0.01591895893216133\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([100, 50], ['ELU', 'ELU'], epochs = 40, lr = 0.05)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d1f5c44-a541-4201-80a9-259eb3d6ac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.863223135471344\n",
      "The validation set accuracy is 0.8075253367424011\n",
      "The training set accuracy is 0.9357438087463379\n",
      "The validation set accuracy is 0.8335745334625244\n",
      "The training set accuracy is 0.9287189841270447\n",
      "The validation set accuracy is 0.8191027641296387\n",
      "The training set accuracy is 0.9334710836410522\n",
      "The validation set accuracy is 0.8277857899665833\n",
      "The training set accuracy is 0.9314049482345581\n",
      "The validation set accuracy is 0.8306801915168762\n"
     ]
    }
   ],
   "source": [
    "for ele in ['sigmoid', 'tanh', 'ReLU', 'leakyReLU', 'ELU']:\n",
    "    NN = NeuralNetwork([50], [ele], epochs = 50, lr = 0.05)\n",
    "    NN.fit(X_train, y_train, scale_X=False)\n",
    "    NN.train()\n",
    "    print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "    print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee81e142-eb06-4e3a-9481-1cb094f2cac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.9084710478782654\n",
      "The validation set accuracy is 0.8046309947967529\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([100, 30], ['tanh', 'sigmoid'], epochs = 50, lr = 0.1)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a79235fd-649d-4f5a-84f8-c9500113c5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.7839850783348083\n",
      "The validation set accuracy is 0.7366136312484741\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([100, 100], ['sigmoid', 'sigmoid'], epochs = 100, lr = 0.1)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))\n",
    "#print('The validation set accuracy is ' + str(NN.get_accuracy(X_test, y_test, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a263e319-3152-4ac4-aa5f-527dbb88c628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.832402229309082\n",
      "The validation set accuracy is 0.730824887752533\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([100, 50], ['tanh', 'tanh'], epochs = 100, lr = 0.1)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00a052e2-7e29-4104-8919-473d2c5e46c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.910330593585968\n",
      "The validation set accuracy is 0.8205499053001404\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([100, 50], ['tanh', 'sigmoid'], epochs = 50, lr = 0.1)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d99bd66-0e45-4063-8f14-b1c30edc3c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.9700413346290588\n",
      "The validation set accuracy is 0.8321273326873779\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([50, 30], ['tanh', 'sigmoid'], epochs = 100, lr = 0.1)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d678ee-11cd-40fd-9cd2-9eb5b8b2eb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.9510330557823181\n",
      "The validation set accuracy is 0.8205499053001404\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([50, 30], ['tanh', 'sigmoid'], epochs = 100, lr = 0.05)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e288e6-7e43-4d79-883c-ba86cf2ef753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.9285123944282532\n",
      "The validation set accuracy is 0.8118668794631958\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([50, 50], ['tanh', 'sigmoid'], epochs = 100, lr = 0.05)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34eb85d4-af79-4f06-bcaf-4a0d051755c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.81688392162323\n",
      "The validation set accuracy is 0.7424023151397705\n",
      "The training set accuracy is 0.8427477478981018\n",
      "The validation set accuracy is 0.730824887752533\n",
      "The training set accuracy is 0.18456445634365082\n",
      "The validation set accuracy is 0.1837916076183319\n",
      "The training set accuracy is 0.18456445634365082\n",
      "The validation set accuracy is 0.1837916076183319\n",
      "The training set accuracy is 0.18456445634365082\n",
      "The validation set accuracy is 0.1837916076183319\n"
     ]
    }
   ],
   "source": [
    "for ele in ['sigmoid', 'tanh', 'ReLU', 'leakyReLU', 'ELU']:\n",
    "    NN = NeuralNetwork([100], [ele], epochs = 100, lr = 0.05)\n",
    "    NN.fit(X_train, y_train, scale_X=False)\n",
    "    NN.train()\n",
    "    print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "    print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5daf8b86-385c-46d7-8778-3bf00cab5280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.7502586245536804\n",
      "The validation set accuracy is 0.7221418023109436\n"
     ]
    }
   ],
   "source": [
    "NN = NeuralNetwork([200, 100], ['sigmoid', 'sigmoid'], epochs = 100, lr = 0.05)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5252a726-e78f-40b9-9f0d-efdcedd717e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.81688392162323\n",
      "The validation set accuracy is 0.7395079731941223\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(292876)\n",
    "NN = NeuralNetwork([150], ['sigmoid'], epochs = 100, lr = 0.05)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0908c1b-5e4c-4a40-81fb-3a1e2b6ffa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.8181253671646118\n",
      "The validation set accuracy is 0.7395079731941223\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(292876)\n",
    "NN = NeuralNetwork([100], ['sigmoid'], epochs = 100, lr = 0.05)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))\n",
    "#print('The test set accuracy is ' + str(NN.get_accuracy(X_test, y_test, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0d36073-cd30-4581-bb72-9b1161dc7306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set accuracy is 0.7219406366348267\n"
     ]
    }
   ],
   "source": [
    "print('The test set accuracy is ' + str(NN.get_accuracy(X_test, y_test, scale_X=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e42215b2-0dc5-4f75-a194-bc48e8542f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is 0.9930511116981506\n",
      "The validation set accuracy is 0.9667987823486328\n",
      "The test set accuracy is 0.9634963870048523\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(292876)\n",
    "NN = NeuralNetwork([100], ['ReLU'], epochs = 100, lr = 0.05)\n",
    "NN.fit(X_train, y_train, scale_X=False)\n",
    "NN.train()\n",
    "print('The training set accuracy is ' + str(NN.get_accuracy()))\n",
    "print('The validation set accuracy is ' + str(NN.get_accuracy(X_validation, y_validation, scale_X=False)))\n",
    "print('The test set accuracy is ' + str(NN.get_accuracy(X_test, y_test, scale_X=False)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
